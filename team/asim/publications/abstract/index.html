<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
</head>
<style>
    body, html {
    font-family: 'Montserrat', sans-serif; /* Montserrat is now the primary font */
    margin: 0;
    padding: 0;
    line-height: 1.6;
    background-color: white;
}

.abstract-container {
    max-width: 1600px; /* Restrict the maximum width */
    margin: 100px auto; /* Center the container */
    padding: 40px; /* Padding inside the container */
}
.title {
    font-size: 24px;
    font-weight: bold;
    margin-bottom: 10px;
}

.date {
    color: #666;
    font-size: 16px;
    margin-bottom: 20px;
}

.abstract-buttons {
     margin-top: 10px;
}
.abstract-container p1 {
        color: #000000; 
        font-size: 50px; /* Set font size */
        font-weight: 600px; /* Make the font bold */
        margin-bottom:50px;
}
.abstract-container .title-container{
    line-height: 1.2;

    justify-content: center; /* Center horizontally */
    align-items: center;     /* Center vertically */
    text-align: left;      /* Ensure text inside is centered */
    margin: auto;            /* Center the container within its parent */
    width: 70%;              /* Limit width to 70% of its parent */


}
    .button-container {
        display: flex;
        align-items: center;
        margin-top: 20px; /* Add some space above the button container */
}

.button, .view-btn {
        display: inline-flex;
        align-items: center;
        padding: 10px 20px;
        margin-right: 10px; /* Space between buttons */
        background-color: #000000; /* Primary button color */
        color: white;
        font-size: 14px;
        font-weight: bold;
        border: none;
        border-radius: 5px;
        cursor: pointer;
        transition: background-color 0.3s, transform 0.2s; /* Smooth transition for hover effects */
        text-decoration: none; /* Removes underline from links if any */
        outline: none; /* Removes default focus outline */
}

.button:hover, .view-btn:hover,
.button:focus, .view-btn:focus {
        background-color: #9c9ea0; /* Darker shade on hover */
        transform: translateY(-2px); /* Slight lift on hover */
}

.icon-arrow {
        margin-left: 10px; /* Space between text and icon */
        height: 20px; /* Consistent icon size */
        width: 20px;
        fill: currentColor; /* Icon color matches the text color */
}

.button:focus, .view-btn:focus {
    border: 2px solid #ffcc00; /* Focus style for accessibility */
}


.share-popup {
    display: none;
    position: fixed;
    left: 50%;
    top: 50%;
    transform: translate(-50%, -50%);
    background-color: #fff;
    padding: 20px;
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    z-index: 1000;
}

.share-popup a, .share-popup abstract-button {
    display: block;
    margin: 5px 0;
}
.abstract-header{
    font-family: 'Georgia', serif; /* Setting Georgia for improved readability */
    font-weight:bold;
font-size:24px;
margin-top:300;
margin-bottom:0;
}
.abstract{
    line-height: 1.3;
    

}

.paragraph-container {
    font-family: 'Georgia', serif; /* Setting Georgia for improved readability */
    font-size: 18px; /* Optimal size for reading on digital screens */
    line-height: 1.6; /* Spacing between lines to enhance readability */
    color: #333; /* Softer black reduces strain on eyes */
    padding: 20px; /* Adds space around the text */
    background-color: #fff; /* A white background to ensure contrast */
    margin: 20px 0; /* Adds vertical space around the paragraph container */
    text-align: justify; /* Justifies the text for a clean, aligned appearance */
    max-width: 700px; /* Limits the width to enhance readability */
    margin-left: auto; /* Centers the container horizontally */
    margin-right: auto;
}


    /* Share button css */

        .centered-flex {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background-color: #FEFEFE;
            font-family: 'Montserrat', sans-serif;
        }
        .social-btn-wrap {
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
            overflow: hidden;
            cursor: pointer;
            width: 180px;
            height: 40px;
            background-color: #EEEEED;
            will-change: transform;
            border-radius: 5px; /* Ensure the inner container also has the same border-radius */

            transition: all .2s ease-in-out;
        }
        .social-btn-wrap:hover {
            transform: scale(1.1);
        }
        .social-btn-label {
            padding-left:20px;
            padding-right:20px;
            position: absolute;
            z-index: 99;
            width: 160px;
            height: 40px;
            font-size: 14px;
            font-weight:bold;
            text-align: center;
            line-height: 40px;            color: #EEEEED;
            background-color: #000000;
            transition: all 1.2s ease;
            transition-delay: .25s;
        }
        .social-icons-container {
            display: flex;
            justify-content: space-around;
            align-items: center;
            width: 180px;
            height: 302px;
            border-radius: 0px;
        }
        .social-icon {
            opacity: 0;
            font-size: 28px;
            color: #1F1E1E;
            transform: scale(.1);
            transition: all .3s ease;
        }
        .social-btn-wrap:hover .social-icon {
            opacity: 1;
            transform: scale(1);
        }
        .social-btn-wrap:hover .social-btn-label {
            transform: translateX(-280px);
        }
        .spacer {
            padding-left:20px;
        }


        /* Navbar css */

    #navbar {
    height: 80px;
    display: flex;
    align-items: center; /* Centers items vertically within the navbar */
    justify-content: space-between; /* Distributes space between and around content items */
    position: fixed; /* Fixed position to keep the navbar at the top on scrolling */
    top: 0;
    left: 0;
    width: 100%; /* Full width of the viewport */
    padding: 10px 40px; /* Padding around the content inside the navbar */
    box-sizing: border-box; /* Includes padding and border in the element's total width and height */
    background-color: white; /* Background color of the navbar */
    z-index: 19; /* Makes sure the navbar is above other content */
    box-shadow: 0 2px 8px 0 rgba(0, 0, 0, 0.2);
    transition: top 0.3s ease-in-out; /* Smooth transition for hiding/showing the navbar */
}

#hidden-nav {
    position: fixed; /* Fixed position */
    z-index: 20; /* Above the main navbar for overlay effect */
    top: 0; /* Aligns to the top of the viewport */
    height: 100%; /* Full height of the viewport */
    width: 300px; /* Width of the hidden navigation */
    background-color: #aaaaaa; /* Background color */
    right: -300px; /* Starts hidden to the right of the viewport */
    transition: right 0.5s; /* Smooth transition for sliding in/out */
    color: white; /* Text color */
    padding: 60px 30px; /* Padding inside the hidden navigation */
    box-sizing: border-box; /* Includes padding in width and height calculations */
    font-family: Arial, Helvetica, sans-serif; /* Font family for text */
    font-size: 20px; /* Font size for text */
    display: flex; /* Enables the use of flexbox properties */
    flex-direction: column; /* Stack items vertically */
    align-items: center; /* Center items horizontally */
    row-gap: 30px; /* Space between items */
}

#hidden-nav > div:hover {
    cursor: pointer; /* Changes the cursor to indicate clickable items */
    opacity: 0.5; /* Transparency effect on hover */
}
.footer-container {
    box-shadow: 0 -4px 8px rgba(0, 0, 0, 0.1); /* Box shadow on top, creating a slight elevation effect */
    text-align: center; /* Center the text inside the footer */
    padding: 20px 0; /* Padding for top and bottom */
    font-size: 14px; /* Appropriate font size for footer text */
    background-color: #f8f9fa; /* Background color for visibility of the shadow */
    width: 100%; /* Ensure it spans the full width of the viewport */
}



@media only screen and (min-width: 720px) {
    #navbar {
        position: fixed; /* Keep the navbar fixed at all viewport sizes */
        top: 0; /* Always at the top */
        width: 100%; /* Full viewport width */
        transition: top 0.3s ease-in-out; /* Smooth transition for vertical movement */
    }
}

                /* For screens with width less than 600px */
        @media only screen and (max-width: 600px) {

        .abstract-container {
            margin-top:120px;
            padding: 20px; /* Smaller padding */
        }
        .abstract-container p1{
            font-size: 25px; /* Reduced margin */
        }

        .title, .date {
            font-size: 15px; /* Smaller font sizes for title and date */
        }

        .paragraph-container {
            font-size: 16px; /* Smaller font size for content */
            padding: 15px; /* Adjust padding */
            max-width: 100%; /* Full width */
        }


        #navbar, #hidden-nav {
            font-size: 14px; /* Smaller font size for navigation */
        }

        .footer-container {
            font-size: 14px; /* Smaller text in footer */
        }
        
        .abstract-header {
            font-size: 18px; /* Smaller font size for abstract header */
        }
    }

</style>
<body>

     <!-- NAVBAR -->
 <div id="navbar" class="noto">
    <img src="logo.svg" class="landing-logo" style="height: 100%;">
    <!-- <p class="research">Research</p> -->
    <div onclick="toggleNav(event)">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" style="width: 30px; height: 30px; cursor: pointer">
            <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5" />
        </svg>              
    </div>
</div>

<div id="hidden-nav"> 
    <div onclick="toggleNav()" style="position: absolute; top: 10px; right: 30px">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" style="width: 20px; height: 20px;">
            <path stroke-linecap="round" stroke-linejoin="round" d="M6 18 18 6M6 6l12 12" />
        </svg>              
    </div>  
    <div onclick="scrollEle('about')">About</div>
    <div onclick="scrollEle('contact-section')">Contact</div>
    <div onclick="scrollEle('team')">Team</div>
</div>



    <div class="abstract-container">
        <div class='title-container'>
            <p1>Publication Title</p1>
        
        <p class="date">Publication Date</p>

        <div class="button-container">
            <button id="viewBtn" class="view-btn" onclick="window.open('https://example.com/publication.pdf', '_blank')">
                View Publication
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon-arrow">
                    <line x1="5" y1="12" x2="19" y2="12"></line>
                    <polyline points="12 5 19 12 12 19"></polyline>
                </svg>
            </button>  
    </div>
    </div>


        <div class="paragraph-container">
        <p class="abstract-header">Abstract</p>
        <p class="abstract">Abstract</p>

        <p>Authors <br> <span class="authors">Authors' names</span> </p><br></p>
    

        <p>Venue<br><span class="venue">Conference Venue</span></p>

    </div>
    
</div>
<footer>
    <div class="footer-container">
        © Tibbling Technologies · All Rights Reserved
    </div>
</footer>
 
    <script src="script.js"></script>
</body>


<!-- js to search from using the id from local storage -->
<script>
    document.addEventListener("DOMContentLoaded", () => {
        const publications = [
        {
            id: 23,  
            date: '2024',
            title: 'Biologically Realistic Computational Primitives of Neocortex Implemented on Neuromorphic Hardware Improve Vision Transformer Performance',
            authors: 'Asim Iqbal, Hassan Mahmood, Greg Stuart, Gordon Fishell, Suraj Honnuraiah',
            conference: 'bioRxiv',
            abstract: 'Understanding the computational principles of the brain and replicating them on neuromorphic hardware and modern deep learning architectures is crucial for advancing neuro-inspired AI (NeuroAI). Here, we develop an experimentally-constrained biophysical network model of neocortical circuit motifs, focusing on layers 2-3 of the primary visual cortex (V1). We investigate the role of four major cortical interneuron classes in a competitive-cooperative computational primitive and validate these circuit motifs implemented soft winner-take-all (sWTA) computation for gain modulation, signal restoration, and context-dependent multistability. Using a novel parameter mapping technique, we configured IBM\'s TrueNorth (TN) chip to implement sWTA computations, mirroring biological neural dynamics. Retrospectively, we observed a strong correspondence between the biophysical model and the TN hardware parameters, particularly in the roles of four key inhibitory neuron classes: Parvalbumin (feedforward inhibition), Somatostatin (feedback inhibition), VIP (disinhibition), and LAMP5 (gain normalization). Moreover, the sparse coupling of this sWTA motif was also able to simulate a two-state neural state machine on the TN chip, replicating working memory dynamics essential for cognitive tasks. Additionally, integrating the sWTA computation as a pre-processing layer in the Vision Transformer (ViT) enhanced its performance on the MNIST digit classification task, demonstrating improved generalization to previously unseen data and suggesting a mechanism akin to zero-shot learning.',
            url: 'https://www.biorxiv.org/content/10.1101/2024.10.06.616839v1.full.pdf'
        },

        {
            id: 22,
            date: '2024',
            title: 'Segment AnyNeuron',
            authors: 'Taha Razzaq, Ahmed Qazi, Asim Iqbal',
            conference: 'bioRxiv',
            abstract: 'Image segmentation plays an integral part in neuroimage analysis and is crucial for understanding brain disorders. Deep Learning (DL) models have shown exponential success in computer vision tasks over the years, including image segmentation. However, to achieve optimal performance, DL models require extensive annotated data for training, which is often the bottleneck to expediting brain-wide image analysis. For segmenting cellular structures such as neurons, the annotation process is cumbersome and time-consuming due to the inherent structural, intensity, and background variations present in the data caused by genetic markers, imaging techniques, etc. We propose an Active Learning-based neuron segmentation framework (Segment AnyNeuron), which incorporates state-of-the-art image segmentation modules - Detectron2 and HQ SAM, and requires minimal ground truth annotation to achieve high precision for brain-wide segmentation of neurons. Our framework can classify and segment completely unseen neuronal data by selecting the most representative samples for manual annotation, thus avoiding the cold-start problem common in Active Learning. We demonstrate the effectiveness of our framework for automated brain-wide segmentation of neurons on a variety of open-source neuron imaging datasets, acquired from different scanners and a variety of transgenic mouse lines.',
            url: 'https://www.biorxiv.org/content/10.1101/2024.08.24.609505v1.full.pdf'
        },

        {
            id: 24,
            date: '2024',
            title: 'NeuReg: Domain-invariant 3D Image Registration on Human and Mouse Brains',
            authors: 'Taha Razzaq, Asim Iqbal',
            conference: 'arXiv',
            abstract: 'Medical brain imaging relies heavily on image registration to accurately curate structural boundaries of brain features for various healthcare applications. Deep learning models have shown remarkable performance in image registration in recent years. Still, they often struggle to handle the diversity of 3D brain volumes, challenged by their structural and contrastive variations and their imaging domains. In this work, we present NeuReg, a Neuro-inspired 3D image registration architecture with the feature of domain invariance. NeuReg generates domain-agnostic representations of imaging features and incorporates a shifting window-based Swin Transformer block as the encoder. This enables our model to capture the variations across brain imaging modalities and species. We demonstrate a new benchmark in multi-domain publicly available datasets comprising human and mouse 3D brain volumes. Extensive experiments reveal that our model (NeuReg) outperforms the existing baseline deep learning-based image registration models and provides a high-performance boost on cross-domain datasets, where models are trained on "source-only" domain and tested on completely "unseen" target domains. Our work establishes a new state-of-the-art for domain-agnostic 3D brain image registration, underpinned by Neuro-inspired Transformer-based architecture.',
            url: 'https://arxiv.org/pdf/2411.06315'
        },

        {
            id: 1,
            date: '2025',
            title: 'An enhancer-AAV toolbox to target and manipulate distinct interneuron subtypes',
            authors: 'Elisabetta Furlanis, Min Dai, Brenda Leyva Garcia, Josselyn Vergara, Ana Pereira, Kenneth Pelkey, Thien Tran, Bram L Gorissen, Anna Vlacho, Ariel Hairston, Shuhan Huang, Deepanjali Dwivedi, Sarah Du, Sara Wills, Justin McMahon, Anthony T Lee, Edward F Chang, Taha Razzaq, Ahmed Qazi, Geoffrey Vargish, Xiaoqing Yuan, Adam Caccavano, Steven Hunt, Ramesh Chittajallu, Nadiya McLean, Lauren Hewit, Emily Paranzino, Haley Rice, Alex C Cummins, Anya Plotnikova, Arya Mohanty, Anne Claire Tangen, Jung Hoon Shin, Reza Azadi, Mark AG Eldridge, Veronica A Alvarez, Bruno B Averbeck, Mansour Alyahyay, Tania Reyes Vallejo, Mohammed Soheib, Lucas G Vattino, Cathryn P MacGregor, Emmie Banks, Viktor Janos Olah, Shovan Naskar, Sophie Hill, Sophie Liebergall, Rohan Badiani, Lili Hyde, Qing Xu, Kathryn C Allaway, Ethan M Goldberg, Tomasz J Nowakowski, Soohyun Lee, Anne E Takesian, Leena A Ibrahim, Asim Iqbal, Chris J McBain, Jordane Dimidschstein, Gord Fishell, Yating Wang',
            conference: 'Neuron (Cell Press)',
            abstract: 'In recent years, we and others have identified a number of enhancers that, when incorporated into rAAV vectors, can restrict the transgene expression to particular neuronal populations. Yet, viral tools to access and manipulate fine neuronal subtypes are still limited. Here, we performed systematic analysis of single cell genomic data to identify enhancer candidates for each of the cortical interneuron subtypes. We established a set of enhancer-AAV tools that are highly specific for distinct cortical interneuron populations and striatal cholinergic neurons. These enhancers, when used in the context of different effectors, can target (fluorescent proteins), observe activity (GCaMP) and manipulate (opto- or chemo-genetics) specific neuronal subtypes. We also validated our enhancer-AAV tools across species. Thus, we provide the field with a powerful set of tools to study neural circuits and functions and to develop precise and targeted therapy',
            url: 'https://www.cell.com/neuron/pdf/S0896-6273(25)00349-6.pdf'
        },

        {
            id: 26,
            date: '2024',
            title: 'Natural Language-guided Neural Encoding Benchmark for Vision',
            authors: 'Taha Razzaq, Hisan Naeem, Asim Iqbal',
            conference: 'The First Workshop on NeuroAI@ NeurIPS2024',
            abstract: 'Understanding the link between visual stimuli and their neural representations is key to advancing Human-Computer Interaction, particularly for therapeutic and assistive technologies. Developing language-guided visual response systems could significantly enhance support for individuals with visual impairments, providing personalized assistance through descriptive language for daily tasks. We present a novel benchmarking approach to evaluate the alignment of image captioning models with neural activity patterns, using a dataset of visual exposures and neural recordings from primates and mice. Our framework systematically maps neural responses from the visual cortex to text tokens generated by image2text models, creating a one-to-one mapping between the most active neurons and generated captions. This cross-species validation demonstrates consistent results across mammalian visual systems, offering both qualitative and quantitative evaluation metrics for benchmarking image2text models based on their neural encoding capabilities in the visual cortex.',
            url: 'https://openreview.net/pdf/bde645ffc2dbe68bee36119f60e072781e4f8dd9.pdf'
         },

        {
            id: 25,
            date: '2024',
            title: 'MortX: A Domain Generalization Benchmark for Mouse Cortex Segmentation and Registration',
            authors: 'Asim Iqbal, Romesa Khan, Edith M. Schneider Gasser, Theofanis Karayannis',
            conference: 'bioRxiv',
            abstract: 'Mesoscale understanding of human brain development is crucial for understanding neurodevelopmental disorders. By applying AI techniques to analyze high-resolution, multi-modal brain imaging datasets across postnatal ages, researchers can study cortical development at the granular level. We introduce MortX, a benchmark dataset of the developing mouse cortex that captures multiple postnatal stages with annotations for distinct anatomical and functional subregions and layers. MortX features high-resolution imaging data including bright-field and fluorescence-labeled neuronal markers. We developed a standardized cortical atlas of genetic markers and manually registered it to brain section images for ground-truth labeling. The dataset serves as a benchmark for domain generalization in neuroimaging, enabling both classical and deep learning models to be trained on source brains and tested on unseen targets. Our results demonstrate generalized model performance and structural invariance across ages. We open-source MortX as a community resource for mouse brain segmentation and registration, emphasizing domain adaptation. This dataset addresses key challenges in mouse brain imaging and advances machine learning models that will help unravel neurodevelopmental disorders.',
            url: 'https://www.biorxiv.org/content/10.1101/2024.11.30.626208v1.full.pdf'
        },
        
        {
            id: 20,
            date: '2024',
            title: 'Multimodal 3D Image Registration for Mapping Brain Disorders',
            authors: 'Hassan Mahmood, Syed Mohammed Shamsul Islam, Asim Iqbal',
            conference: 'bioRxiv',
            abstract: 'We introduce an AI-driven approach for robust 3D brain image registration, addressing challenges posed by diverse hardware scanners and imaging sites. Our model trained using an SSIM-driven loss function, prioritizes structural coherence over voxel-wise intensity matching, making it uniquely robust to inter-scanner and intra-modality variations. This innovative end-to-end framework combines global alignment and non-rigid registration modules, specifically designed to handle structural, intensity, and domain variances in 3D brain imaging data. Our approach outperforms the baseline model in handling these shifts, achieving results that align closely with clinical ground-truth measurements. We demonstrate its efficacy on 3D brain data from healthy individuals and dementia patients, with particular success in quantifying brain atrophy, a key biomarker for Alzheimer’s disease and other brain disorders. By effectively managing variability in multisite, multi-scanner neuroimaging studies, our approach enhances the precision of atrophy measurements for clinical trials and longitudinal studies. This advancement promises to improve diagnostic and prognostic capabilities for neurodegenerative disorders.',
            url: 'https://www.biorxiv.org/content/10.1101/2024.08.24.609508v1.full.pdf'
        },

        {
            id: 21,
            date: '2024',
            title: 'NeuroAtlas: An Artificial Intelligence-based Framework for Annotation, Segmentation and Registration of Large Scale Biomedical Imaging Data',
            authors: 'Hassan Mahmood, Farah Nawar, Syed Mohammed Shamsul Islam, Asim Iqbal',
            conference: 'bioRxiv',
            abstract: 'With increasing neuroimaging modalities and data diversity, mapping brain regions to a standard atlas template has become a challenging problem. Machine learning in general and deep learning, in particular, have been providing robust solutions for several neuroimaging tasks, including brain image registration and segmentation. However, these methods require a large amount of data for groundtruth labels, annotated by human experts, which is time-consuming. In this work, we introduce NeuroAtlas, an AI-based framework for atlas generation and brain region segmentation. We showcase an end-to-end solution for brain registration and segmentation by providing i) a deep learning modeling suite with a variety of high-performing model architectures to map a brain atlas onto the input brain section and ii) a Graphical User Interface (GUI)-based plugin for large-scale data annotation with a feature of modifying the predicted labels for active learning. We demonstrate a robust performance of our framework on the human brains, captured through various imaging modalities and age groups, and demonstrate its application for mouse brains as well. NeuroAtlas tool will be open-sourced and entirely compatible with both local as well as cloud-based computing so that users can easily adapt to their neuroimaging custom datasets.',
            url: 'https://www.biorxiv.org/content/10.1101/2024.08.24.609507v1.full.pdf'
        },

        {
            id: 2,
            date: '2024',
            title: 'AnimalFormer: Multimodal Vision Framework for Behavior-based Precision Livestock Farming',
            authors: 'Ahmed Qazi, Taha Razzaq, Asim Iqbal',
            conference: 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition',
            abstract: 'We introduce a multimodal vision framework for precision livestock farming, harnessing the power of GroundingDINO, HQSAM, and ViTPose models. This integrated suite enables comprehensive behavioral analytics from video data without invasive animal tagging. GroundingDINO generates accurate bounding boxes around livestock, while HQSAM segments individual animals within these boxes. ViTPose estimates key body points, facilitating posture and movement analysis. Demonstrated on a sheep dataset with grazing, running, sitting, standing, and walking activities, our framework extracts invaluable insights:activity and grazing patterns, interaction dynamics, and detailed postural evaluations. Applicable across species and video resolutions, this framework revolutionizes noninvasive livestock monitoring for activity detection, counting, health assessments, and posture analyses. It empowers data-driven farm management, optimizing animal welfare and productivity through AI-powered behavioral understanding.',
            url: 'https://openaccess.thecvf.com/content/CVPR2024W/Vision4Ag/papers/Qazi_AnimalFormer_Multimodal_Vision_Framework_for_Behavior-based_Precision_Livestock_Farming_CVPRW_2024_paper.pdf'
        },

        {
            id: 3,
            date: '2024',
            title: 'ExerAIde: AI-assisted Multimodal Diagnosis for Enhanced Sports Performance and Personalised Rehabilitation',
            authors: 'Ahmed Qazi, Asim Iqbal',
            conference: 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition',
            abstract: 'The quest for personalized sports therapy has long been a concern for practitioners and patients alike, aiming for recovery protocols that transcend the one-size-fits-all approach. In this study, we introduce a novel framework for personalized sports therapy through automated joint movement analysis. By synthesizing the analytical capabilities of a Random Forest Classifier (RFC) with a Vector Quantized Variational AutoEncoder (VQ-VAE), we systematically discern the nuanced kinematic differences between healthy and pathological exercise movements. The RFC prioritizes the joints by their discriminative influence on movement healthiness, which informs the VQ-VAE’s derivation of a distilled list of pivotal joints. This dual-model approach not only identifies a hierarchy of joint importance but also ascertains the minimal subset of joints critical for distinguishing between healthy and unhealthy movement patterns. The resultant data-driven insight into joint-specific dynamics underpins the development of targeted, individualized rehabilitation programs. Our results exhibit promising directions in sports therapy, showcasing the potential of machine learning in developing personalized therapeutic interventions.',
            url: 'https://openaccess.thecvf.com/content/CVPR2024W/CVsports/papers/Qazi_ExerAIde_AI-assisted_Multimodal_Diagnosis_for_Enhanced_Sports_Performance_and_Personalised_CVPRW_2024_paper.pdf'
        },
        {
            id: 4,
            date: '2024',
            title: 'CellSeg3D: self-supervised 3D cell segmentation for microscopy',
            authors: 'Cyril Achard, Timokleia Kousi, Markus Frey, Maxime Vidal, Yves Paychere, Colin Hofmann, Asim Iqbal, Sebastien B Hausmann, Stephane Pages, Mackenzie W Mathis',
            conference: 'eLife',
            abstract: 'Understanding the complex three-dimensional structure of cells is crucial across many disciplines in biology and especially in neuroscience. Here, we introduce a novel 3D self-supervised learning method designed to address the inherent complexity of quantifying cells in 3D volumes, often in cleared neural tissue. We offer a new 3D mesoSPIM dataset and show that CellSeg3D can match state-of-the-art supervised methods. Our contributions are made accessible through a Python package with full GUI integration in napari.',
            url: 'https://elifesciences.org/reviewed-preprints/99848v1'
        },
        {
            id: 5,
            date: '2023',
            title: '3D Brain Registration with Intensity Shift Robustness',
            authors: 'Hassan Mahmood, Asim Iqbal, Syed Mohammed Shamsul Islam, Syed Afaq Ali Shah',
            conference: 'IEEE International Conference on Image Processing (ICIP)',
            abstract: 'Technological advances in medical imaging are enabling us to understand healthcare datasets in great detail. Machine Learning enabled methods, specifically, deep neural networks are continuously achieving benchmark performances in terms of accuracy and computational efficiency. However, the lack of agreed-upon standard procedures, variations in the devices by different vendors, and artifacts induced by the physical phenomenon in the sensors make the data inconsistent and noisy. These variations in the data are detrimental to the performance of learning-based methods. In this study, we analyze the behavior of traditional and deep learning-based image registration methods and explore strategies to handle the problem of intensity distributional shifts without compromising the performance. To achieve this, we propose an intensity-based loss function and demonstrate that the models trained with our proposed loss function are better at handling unseen data from different sites using machines from different vendors. In addition, our trained model is superior in preserving the boundaries of anatomical regions after registration.',
            url: 'https://ieeexplore.ieee.org/abstract/document/10222341'
        },
        {
            id: 6,
            date: '2023',
            title: 'Domain-Invariant Brainstem Nuclei Segmentation and Signal Quantification',
            authors: 'Julia Kaiser, Dana Luong, Eunseo Sung, Asim Iqbal, Vibhu Sahni',
            conference: 'bioRxiv',
            abstract: 'Brainstem nuclei are hard to distinguish due to very few distinctive features which makes detecting them with high accuracy extremely difficult. We introduce StARQ that builds on SeBRe, a deep learning-based framework to segment regions of interest. StARQ provides new functionalities for automated segmentation of brainstem nuclei at high granularity, and quantification of underlying neural features such as axonal tracings, and synaptic punctae. StARQ will serve as a toolbox for generalized brainstem analysis, enabling reliable high-throughput computational analysis with open-source models.',
            url: 'https://www.biorxiv.org/content/biorxiv/early/2023/11/11/2023.11.07.566040.full.pdf'
        },
        {
            id: 8,
            date: '2022',
            title: 'Processing time-domain and frequency-domain representations of eeg data',
            authors: 'Asim Iqbal, Garrett Raymond Honke, Nina Thigpen, Vladimir Miskovic, Pramod Gupta',
            conference: 'US Patent',
            abstract: 'Methods, systems, and apparatus, including computer programs encoded on computer storage media, for processing representations of EEG measurements. One of the methods includes obtaining a plurality of EEG signal measurements corresponding to respective EEG trials of a user; generating a time-domain representation from the plurality of EEG signal measurements, where the time-domain representation includes a plurality of rows, and where each row corresponds to a different set of one or more EEG signal measurements; applying the time-domain representation as input to a neural network having a plurality of network parameters, final values of the network parameters having been determined by a transfer learning process where the neural network is initially trained to perform an image processing task and the neural network is subsequently trained to perform EEG analysis; and obtaining, from the neural network, a mental health prediction for the user.',
            url: 'https://patentimages.storage.googleapis.com/41/21/e2/62f11998fd2b54/US20220101997A1.pdf'
        },
        {
            id: 9,
            date: '2022',
            title: 'Processing time-frequency representations of eeg data using neural networks',
            authors: 'Asim Iqbal, Pramod Gupta, Garrett Raymond Honke, Vladimir Miskovic',
            conference: 'US Patent',
            abstract: 'Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating embeddings of EEG measurements. One of the methods includes obtaining a two-dimensional time-frequency electroencephalogram (EEG) representation corresponding to one or more EEG signal measurements of a user; processing the time-frequency EEG representation using a first neural network having a plurality of first network parameters to generate an embedding of the time-frequency EEG representation, wherein the first neural network has been trained using transfer learning; and providing the embedding of the time-frequency EEG representation to a downstream neural network to generate a mental health prediction for the user.',
            url: 'https://patentimages.storage.googleapis.com/03/34/73/6f833899313f8d/US20220015659A1.pdf'
        },
        {
            id: 10,
            date: '2021',
            title: 'An automatic multi-tissue human fetal brain segmentation benchmark using the fetal tissue annotation dataset',
            authors: 'Kelly Payette, Priscille de Dumast, Hamza Kebiri, Ivan Ezhov, Johannes C Paetzold, Suprosanna Shit, Asim Iqbal, Romesa Khan, Raimund Kottke, Patrice Grehten, Hui Ji, Levente Lanczi, Marianna Nagy, Monika Beresova, Thi Dao Nguyen, Giancarlo Natalucci, Theofanis Karayannis, Bjoern Menze, Meritxell Bach Cuadra, Andras Jakab',
            conference: 'Nature Scientific Data',
            abstract: "It is critical to quantitatively analyse the developing human fetal brain in order to fully understand neurodevelopment in both normal fetuses and those with congenital disorders. To facilitate this analysis, automatic multi-tissue fetal brain segmentation algorithms are needed, which in turn requires open datasets of segmented fetal brains. Here we introduce a publicly available dataset of 50 manually segmented pathological and non-pathological fetal magnetic resonance brain volume reconstructions across a range of gestational ages (20 to 33 weeks) into 7 different tissue categories (external cerebrospinal fluid, grey matter, white matter, ventricles, cerebellum, deep grey matter, brainstem/spinal cord). In addition, we quantitatively evaluate the accuracy of several automatic multi-tissue segmentation algorithms of the developing human fetal brain. Four research groups participated, submitting a total of 10 algorithms, demonstrating the benefits the dataset for the development of automatic algorithms.",
            url: 'https://www.nature.com/articles/s41597-021-00946-3.pdf'
        },
        {
            id: 12,
            date: '2020',
            title: 'Exploring intensity invariance in deep neural networks for brain image registration',
            authors: 'Hassan Mahmood, Asim Iqbal, Syed Mohammed Shamsul Islam',
            conference: 'IEEE Digital Image Computing: Techniques and Applications (DICTA)',
            abstract: "Image registration is a widely-used technique in analysing large scale datasets that are captured through various imaging modalities and techniques in biomedical imaging such as MRI, X-Rays, etc. These datasets are typically collected from various sites and under different imaging protocols using a variety of scanners. Such heterogeneity in the data collection process causes inhomogeneity or variation in intensity (brightness) and noise distribution. These variations play a detrimental role in the performance of image registration, segmentation and detection algorithms. Classical image registration methods are computationally expensive but are able to handle these artifacts relatively better. However, deep learning-based techniques are shown to be computationally efficient for automated brain registration but are sensitive to the intensity variations. In this study, we investigate the effect of variation in intensity distribution among input image pairs for deep learning-based image registration methods. We find a performance degradation of these models when brain image pairs with different intensity distribution are presented even with similar structures. To overcome this limitation, we incorporate a structural similarity-based loss function in a deep neural network and test its performance on the validation split separated before training as well as on a completely unseen new dataset. We report that the deep learning models trained with structure similarity-based loss seems to perform better for both datasets. This investigation highlights a possible performance limiting factor in deep learning-based registration models and suggests a potential solution to incorporate the intensity distribution variation in the input image pairs. Our code and models are available at https://github.com/hassaanmahmood/DeepIntense.",
            url: 'https://arxiv.org/pdf/2009.10058'
        },
        {
            id: 13,
            date: '2020',
            title: 'Developmental divergence of sensory stimulus representation in cortical interneurons',
            authors: 'Rahel Kastli, Rasmus Vighagen, Alexander van der Bourg, Ali Ozgur Argunsah, Asim Iqbal, Fabian F Voigt, Daniel Kirschenbaum, Adriano Aguzzi, Fritjof Helmchen, Theofanis Karayannis',
            conference: 'Nature Communications',
            abstract: "Vasocative-intestinal-peptide (VIP+) and somatostatin (SST+) interneurons are involved in modulating barrel cortex activity and perception during active whisking. Here we identify a developmental transition point of structural and functional rearrangements onto these interneurons around the start of active sensation at P14. Using in vivo two-photon Ca2+ imaging, we find that before P14, both interneuron types respond stronger to a multi-whisker stimulus, whereas after P14 their responses diverge, with VIP+ cells losing their multi-whisker preference and SST+ neurons enhancing theirs. Additionally, we find that Ca2+ signaling dynamics increase in precision as the cells and network mature. Rabies virus tracings followed by tissue clearing, as well as photostimulation-coupled electrophysiology reveal that SST+ cells receive higher cross-barrel inputs compared to VIP+ neurons at both time points. In addition, whereas prior to P14 both cell types receive direct input from the sensory thalamus, after P14 VIP+ cells show reduced inputs and SST+ cells largely shift to motor-related thalamic nuclei.",
            url: 'https://www.nature.com/articles/s41467-020-19427-z.pdf'
        },
        {
            id: 14,
            date: '2020',
            title: 'A comparison of automatic multi-tissue segmentation methods of the human fetal brain using the FeTA dataset',
            authors: 'Kelly Payette, Priscille de Dumast, Hamza Kebiri, Ivan Ezhov, Johannes C Paetzold, Suprosanna Shit, Asim Iqbal, Romesa Khan, Raimund Kottke, Patrice Grehten, Hui Ji, Levente Lanczi, Marianna Nagy, Monika Beresova, Thi Dao Nguyen, Giancarlo Natalucci, Theofanis Karayannis, Bjoern Menze, Meritxell Bach Cuadra, Andras Jakab',
            conference: 'arXiv',
            abstract: "It is critical to quantitatively analyse the developing human fetal brain in order to fully understand neurodevelopment in both normal fetuses and those with congenital disorders. To facilitate this analysis, automatic multi-tissue fetal brain segmentation algorithms are needed, which in turn requires open databases of segmented fetal brains. Here we introduce a publicly available database of 50 manually segmented pathological and non-pathological fetal magnetic resonance brain volume reconstructions across a range of gestational ages (20 to 33 weeks) into 7 different tissue categories (external cerebrospinal fluid, grey matter, white matter, ventricles, cerebellum, deep grey matter, brainstem/spinal cord). In addition, we quantitatively evaluate the accuracy of several automatic multi-tissue segmentation algorithms of the developing human fetal brain. Four research groups participated, submitting a total of 10 algorithms, demonstrating the benefits the database for the development of automatic algorithms.",
            url: 'https://www.arxiv.org/pdf/2010.15526'
        },
        {
            id: 15,
            date: '2019',
            title: 'DeNeRD: high-throughput detection of neurons for brain-wide analysis with deep learning',
            authors: 'Asim Iqbal, Asfandyar Sheikh, Theofanis Karayannis',
            conference: 'Nature Publishing Group',
            abstract: "Mapping the structure of the mammalian brain at cellular resolution is a challenging task and one that requires capturing key anatomical features at the appropriate level of analysis. Although neuroscientific methods have managed to provide significant insights at the micro and macro level, in order to obtain a whole-brain analysis at a cellular resolution requires a meso-scopic approach. A number of methods can be currently used to detect and count cells, with, nevertheless, significant limitations when analyzing data of high complexity. To overcome some of these constraints, we introduce a fully automated Artificial Intelligence (AI)-based method for whole-brain image processing to Detect Neurons in different brain Regions during Development (DeNeRD). We demonstrate a high performance of our deep neural network in detecting neurons labeled with different genetic markers in a range of imaging planes and imaging modalities.",
            url: 'https://www.nature.com/articles/s41598-019-50137-9.pdf'
        },
        {
            id: 16,
            date: '2019',
            title: 'A deeply learned brain atlas',
            authors: 'Nina Vogt, Asim Iqbal, Chen Yuncong',
            conference: 'Nature Methods',
            abstract: "Segmenting and registration of brain imaging datasets can be a tedious and time-consuming task. Iqbal et al. now use a deep learning approach, which they call SeBRe, to facilitate this task for Nissl-stained, fluorescence, and even magnetic resonance image datasets. They trained a deep neural network to segment and classify eight different regions in the mouse brain. After training, SeBRe could segment and register other datasets with a precision of 0.84, and similar performance could be achieved even if the brains were stained for previously unseen markers or imaged with a different microscopy modality. While SeBRe registers the image datasets to an existing brain atlas such as the Allen Brain Atlas, Chen et al. went a step further. They used convolutional neural networks to build a mouse brain atlas from scratch. This brain atlas is active and can be augmented with additional datasets. Furthermore, it preserves information on the variance between datasets. Both pipelines automate the processing of anatomical brain datasets and should substantially speed up the mapping of neurons and brain regions.",
            url: 'https://www.nature.com/articles/s41592-019-0522-8.pdf'
        },
        {
            id: 17,
            date: '2019',
            title: 'Decoding neural responses in mouse visual cortex through a deep neural network',
            authors: 'Asim Iqbal, Phil Dong, Christopher M Kim, Heeun Jang',
            conference: 'IEEE International Joint Conference on Neural Networks (IJCNN)',
            abstract: "Finding a code to unravel the population of neural responses that leads to a distinct animal behavior has been a long-standing question in the field of neuroscience. With the recent advances in machine learning, it is shown that the hierarchically Deep Neural Networks (DNNs) perform optimally in decoding unique features out of complex datasets. In this study, we utilize the power of a DNN to explore the computational principles in the mammalian brain by exploiting the Neuropixel data from Allen Brain Institute. We decode the neural responses from mouse visual cortex to predict the presented stimuli to the animal for natural (bear, trees, cheetah, etc.) and artificial (drifted gratings, orientated bars, etc.) classes. Our results indicate that neurons in mouse visual cortex encode the features of natural and artificial objects in a distinct manner, and such neural code is consistent across animals. We investigate this by applying transfer learning to train a DNN on the neural responses of a single animal and test its generalized performance across multiple animals. Within a single animal, DNN is able to decode the neural responses with as much as 100% classification accuracy. Across animals, this accuracy is reduced to 91%. This study demonstrates the potential of utilizing the DNN models as a computational framework to understand the neural coding principles in the mammalian brain.",
            url: 'https://arxiv.org/pdf/1911.05479'
        },
        {
            id: 18,
            date: '2019',
            title: 'Developing a brain atlas through deep learning',
            authors: 'Asim Iqbal, Romesa Khan, Theofanis Karayannis',
            conference: 'Nature Machine Intelligence',
            abstract: "Neuroscientists have devoted significant effort into the creation of standard brain reference atlases for high-throughput registration of anatomical regions of interest. However, variability in brain size and form across individuals poses a significant challenge for such reference atlases. To overcome these limitations, we introduce a fully automated deep neural networkbased method (SeBRe) for registration through Segmenting Brain Regions of interest with minimal human supervision. We demonstrate the validity of our method on brain images from different mouse developmental time points, across a range of neuronal markers and imaging modalities. We further assess the performance of our method on images from MR-scanned human brains. Our registration method can accelerate brain-wide exploration of region-specific changes in brain development and, by simply segmenting brain regions of interest for highthroughput brain-wide analysis, provides an alternative to existing complex brain registration techniques.",
            url: 'https://rdcu.be/b4DfW'
        },
        {
            id: 19,
            date: '2018',
            title: 'Exploring brain-wide development of inhibition through deep learning',
            authors: 'Asim Iqbal, Asfandyar Sheikh, Theofanis Karayannis',
            conference: 'arXiv',
            abstract: 'We introduce here a fully automated convolutional neural network-based method for brain image processing to Detect Neurons in different brain Regions during Development (DeNeRD). Our method takes a developing mouse brain as input and i) registers the brain sections against a developing mouse reference atlas, ii) detects various types of neurons, and iii) quantifies the neural density in many unique brain regions at different postnatal (P) time points. Our method is invariant to the shape, size and expression of neurons and by using DeNeRD, we compare the brain-wide neural density of all GABAergic neurons in developing brains of ages P4, P14 and P56. We discover and report 6 different clusters of regions in the mouse brain in which GABAergic neurons develop in a differential manner from early age (P4) to adulthood (P56). These clusters reveal key steps of GABAergic cell development that seem to track with the functional development of diverse brain regions as the mouse transitions from a passive receiver of sensory information (<P14) to an active seeker (>P14).',
            url: 'https://arxiv.org/pdf/1807.03238'
        },



];

                

            const selectedPublicationId = sessionStorage.getItem('selectedPublicationId');  // Retrieve from sessionStorage

const publication = publications.find(pub => pub.id === Number(selectedPublicationId));

if (publication) {
    document.querySelector('p1').textContent = publication.title;
    document.querySelector('.date').textContent = publication.date;
    document.querySelector('.authors').textContent = publication.authors;
    document.querySelector('.venue').textContent = publication.conference;
    document.querySelector('.abstract').textContent = publication.abstract;
          // Set the URL for the View and Download buttons
        const viewButton = document.getElementById('viewBtn');

        // Update View button to open the publication in a new tab
        viewButton.onclick = () => window.open(publication.url, '_blank');

        // Set the href for the download button

} else {
    document.querySelector('.abstract-container').innerHTML = '<p>No publication data available.</p>';
}
});

</script>





 <!-- Javascript for navbar -->
 <script>
    document.addEventListener("DOMContentLoaded", () => {
let lastScrollTop = 0; // Variable to store the last scroll position
const navbar = document.getElementById('navbar'); // Get the navbar element

window.addEventListener("scroll", function() {
    let scrollTop = window.pageYOffset || document.documentElement.scrollTop;
    
    if (scrollTop > lastScrollTop) {
        // Downscroll code
        navbar.style.top = "-80px"; // Adjust this value to match the navbar's height
    } else {
        // Upscroll code
        navbar.style.top = "0px";
    }
    lastScrollTop = scrollTop <= 0 ? 0 : scrollTop; // Update lastScrollTop to current position
}, false);
});

    var nav_open = false
    let hidden_nav = document.getElementById("hidden-nav");

    document.addEventListener('click', (e) => {
        if (nav_open && !hidden_nav.contains(e.target)) {
            toggleNav();
        }
    });

    function toggleNav(e) {
        nav_open = hidden_nav.style.right == "0px";
        hidden_nav.style.right = nav_open ? "-300px" : "0px";
        nav_open = !nav_open;
        e?.stopPropagation();
    }

    function scrollEle(ele) {
        toggleNav();
        document.getElementById(ele).scrollIntoView({behavior: 'smooth', block: "center", inline: "nearest"});
    }
</script>


<!-- Javascript for share button -->

<script>
    document.addEventListener("DOMContentLoaded", function() {
        const publications = [
            {id: 1, url: "http://example.com/publication1"},
            {id: 2, url: "http://example.com/publication2"}
        ];
        const selectedPublicationId = sessionStorage.getItem('selectedPublicationId'); // Retrieve from sessionStorage
        const publication = publications.find(pub => pub.id === Number(selectedPublicationId)); // Convert to Number if stored as String

        if (publication) {
            // Setting Facebook share URL
            document.getElementById('shareFacebook').href = `https://www.facebook.com/sharer/sharer.php?u=${encodeURIComponent(publication.url)}`;

            // Setting Twitter share URL
            document.getElementById('shareTwitter').href = `https://twitter.com/intent/tweet?text=${encodeURIComponent('Check out this interesting publication!')}&url=${encodeURIComponent(publication.url)}`;
        } else {
            console.log('Publication not found or session storage key missing');
        }
    });
</script>
</html>
